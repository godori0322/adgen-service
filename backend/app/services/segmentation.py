# segmentation.py
# SAM ë‹¨ë…ìœ¼ë¡œ ì…ë ¥ë°›ì€ ì´ë¯¸ì§€ì—ì„œ ì œí’ˆ ëˆ„ë¼ ë”°ê¸°

# .env ë¡œë“œ (SAM_MODEL_PATH ì‚¬ìš©)
from dotenv import load_dotenv
load_dotenv()

import os
import threading
import torch
import numpy as np
import cv2
from PIL import Image
from io import BytesIO
import base64

from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator

_segmentation_singleton = None

class ProductSegmentation:
    def __init__(
        self,
        sam_model_type: str = "vit_b",
        sam_max_size: int = 1024,   # SAM ì…ë ¥ ìµœëŒ€ í•´ìƒë„(ê¸´ ë³€ ê¸°ì¤€, í•„ìš” ì‹œ ì‚¬ìš©)
        points_per_side: int = 24, # ìë™ ë§ˆìŠ¤í¬ ìƒì„± ì •ë°€ë„ (í•„ìš”ì‹œ ì¡°ì ˆ)
        upscaler_scale: int = 2,
    ):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.sam_model_type = sam_model_type
        self.sam_max_size = sam_max_size
        self.points_per_side = points_per_side
        self.upscale_factor = upscaler_scale

        # lazy loadingìš© í”Œë ˆì´ìŠ¤í™€ë”
        self.sam_model = None
        self.sam_predictor = None
        self.mask_gen = None

        self._load_lock = threading.Lock()

    # =========================================================
    # 1. SAM ëª¨ë¸ ë¡œë“œ (Lazy Load)
    # =========================================================
    def _ensure_models_loaded(self):
        if self.sam_model is not None:
            return

        with self._load_lock:
            if self.sam_model is not None:
                return

            sam_ckpt = os.getenv("SAM_MODEL_PATH")
            if sam_ckpt is None or not os.path.isfile(sam_ckpt):
                raise FileNotFoundError("SAM_MODEL_PATH í™˜ê²½ë³€ìˆ˜ë¥¼ í†µí•´ SAM ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            print(f"[Segmentation] Loading SAM ({self.sam_model_type}) from {sam_ckpt}")

            # SAM ëª¨ë¸ ë¡œë“œ
            sam_model = sam_model_registry[self.sam_model_type](checkpoint=sam_ckpt)
            sam_model.to("cuda")
            sam_model.eval()

            # Predictor (ì›í•˜ë©´ point/box prompt ìš©ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥)
            self.sam_predictor = SamPredictor(sam_model)

            # SAM ì „ìš© AutomaticMaskGenerator ì‚¬ìš©
            self.mask_gen = SamAutomaticMaskGenerator(
                sam_model,
                points_per_side=self.points_per_side,
                pred_iou_thresh=0.88,
                stability_score_thresh=0.9,
                crop_n_layers=1,
                crop_n_points_downscale_factor=2,
                min_mask_region_area=200,  # ì‘ì€ ë…¸ì´ì¦ˆ ì œê±°
            )

            self.sam_model = sam_model
            print("[Segmentation] SAM ëª¨ë¸ ë° ìë™ ë§ˆìŠ¤í¬ ì œë„ˆë ˆì´í„° ë¡œë“œ ì™„ë£Œ.")

    # =========================================================
    # 4. ìœ í‹¸ â€” ì•ˆì „ ë¦¬ì‚¬ì´ì§•
    # =========================================================
    def _resize_for_sam(self, image: Image.Image):
        w, h = image.size
        max_side = max(w, h)
        if max_side <= self.sam_max_size:
            return image

        scale = self.sam_max_size / max_side
        new_w = int(w * scale)
        new_h = int(h * scale)
        return image.resize((new_w, new_h), Image.LANCZOS)

    # =========================================================
    # 2. PUBLIC API â€” ìµœì¢… ëˆ„ë¼ (SAM ë‹¨ë…)
    # =========================================================
    def remove_background(self, image: Image.Image):
        """
        SAMë§Œ ì‚¬ìš©í•´ì„œ:
        1) ìë™ ë§ˆìŠ¤í¬ ìƒì„±
        2) ê°€ì¥ í° ê°ì²´ì˜ ë§ˆìŠ¤í¬ ì„ íƒ
        3) RGBA ì»·ì•„ì›ƒ ìƒì„±

        return:
            refined_mask (np.ndarray, 0~1 float)
            rgba_cutout (PIL.Image, RGBA)
        """
        self._ensure_models_loaded()

        # ğŸ”¥ SAM-safe resize
        image = self._resize_for_sam(image)

        img_rgb = np.array(image.convert("RGB"))

        # SAMì˜ AutomaticMaskGeneratorë¡œ ë§ˆìŠ¤í¬ í›„ë³´ ìƒì„±
        masks = self.mask_gen.generate(img_rgb)
        if len(masks) == 0:
            raise ValueError("SAMì´ ë§ˆìŠ¤í¬ë¥¼ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()

        # ì¢…í•© ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ì œí’ˆì¼ í™•ë¥ ì´ ë†’ì€ ë§ˆìŠ¤í¬ ì„ ì •
        best_mask = select_best_mask(img_rgb, masks)

        # inversion ì²´í¬ (ì¤‘ì•™ì´ ë¹„ì–´ ìˆìœ¼ë©´ ë°˜ì „)
        if mask_needs_invert(best_mask):
            best_mask = 1 - best_mask

        # ê²½ê³„ ë¶€ë“œëŸ½ê²Œ (0~1 float)
        refined_mask = refine_mask(best_mask)

        # maskë¥¼ ë‹¤ì‹œ hard-edge maskë¡œ sharpen
        sharp_mask = (refined_mask > 0.5).astype(np.float32)

        # halo ì œê±°
        # refined_mask = remove_halo(refined_mask)

        # Guided Filter ì ìš© (edge ë³´ì¡´ ìµœê³  íš¨ê³¼)
        gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)
        guided = guided_filter(gray, sharp_mask, r=6, eps=1e-4)

        guided = np.clip(guided, 0, 1)

        # Final RGBA cutout
        alpha = (guided * 255).astype(np.uint8)
        rgba = np.dstack([img_rgb, alpha])
        rgba_final = Image.fromarray(rgba)

        return guided, rgba_final

    # =========================================================
    # 3. ìœ í‹¸ â€” RGBA cutout ìƒì„±
    # =========================================================
    @staticmethod
    def _create_cutout(img_np: np.ndarray, mask_float: np.ndarray) -> Image.Image:
        """
        img_np: H x W x 3 (RGB, uint8)
        mask_float: H x W (0~1 float)
        """
        alpha = np.clip(mask_float * 255.0, 0, 255).astype(np.uint8)
        rgba = np.dstack([img_np, alpha])
        return Image.fromarray(rgba)

# -------------------------------------------------------------
# Singleton instance (ì„œë²„ ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰, ë™ì¼ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©)
# -------------------------------------------------------------
def get_segmentation_singleton():
    global _segmentation_singleton
    if _segmentation_singleton is None:
        _segmentation_singleton = ProductSegmentation()
        _segmentation_singleton._ensure_models_loaded()
    return _segmentation_singleton

# =============================================================
# 4. inversion ì²´í¬
# =============================================================
def mask_needs_invert(mask: np.ndarray) -> bool:
    """
    ë§ˆìŠ¤í¬ ì¤‘ì•™ ë¶€ë¶„ì´ ëŒ€ë¶€ë¶„ 0ì´ë©´ (ë°°ê²½ìœ¼ë¡œ íŒë‹¨ë˜ë©´) ë°˜ì „ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨.
    """
    if mask.ndim == 3:
        mask = mask[..., 0]

    h, w = mask.shape

    # ì¤‘ì•™ 40% ì˜ì—­ ì²´í¬
    y1, y2 = int(h * 0.3), int(h * 0.7)
    x1, x2 = int(w * 0.3), int(w * 0.7)

    center_ratio = np.mean(mask[y1:y2, x1:x2])

    # ì „ì²´ í”½ì…€ ì¤‘ ë§ˆìŠ¤í¬ ë¹„ìœ¨
    fg_ratio = np.mean(mask)

    # ì¡°ê±´ 1: ì¤‘ì•™ì´ ë¹„ì–´ ìˆìŒ (ê¸°ì¡´ ë¡œì§)
    cond1 = center_ratio < 0.25

    # ì¡°ê±´ 2: ê±°ì˜ ì „ì²´ê°€ ë§ˆìŠ¤í¬ (ë°°ê²½ ì „ì²´ ê°ì§€í•œ ê²½ìš°)
    cond2 = fg_ratio > 0.9

    return cond1 or cond2


# =============================================================
# 5. ê²½ê³„ ë¶€ë“œëŸ½ê²Œ
# =============================================================
def refine_mask(mask: np.ndarray, blur_size: int = 5) -> np.ndarray:
    """
    mask: 0/1 (uint8 ë˜ëŠ” bool)
    return: 0~1 float mask (blur ì ìš©)
    """
    mask = (mask * 255).astype(np.uint8)
    mask = cv2.GaussianBlur(mask, (blur_size, blur_size), 0)
    return (mask.astype(np.float32) / 255.0)

# =============================================================
# 6. ê²½ê³„ì˜ halo ì œê±°
# =============================================================
def remove_halo(mask_float: np.ndarray, blur_size=5):
    """
    SAMì´ ë§Œë“  mask_float(0-1)ì„ ì…ë ¥ë°›ì•„ halo ì œê±° í›„ ë°˜í™˜
    """
    mask = (mask_float * 255).astype(np.uint8)

    # morphological closingìœ¼ë¡œ ì‘ì€ í‹ˆ ë©”ê¾¸ê¸° (ê²½ê³„ ë³´ì¡´)
    kernel = np.ones((3, 3), np.uint8)
    closed = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)

    # ì•½í•œ blurë¡œ smoothing
    cleaned = cv2.GaussianBlur(closed, (blur_size, blur_size), 0)

    return cleaned.astype(np.float32) / 255.0

# =============================================================
# 7. ìƒ‰ìƒ decontamination
# =============================================================
def color_decontaminate(img_np, mask_float, strength=0.1):
    """
    img_np: HXW RGB
    mask_float: 0-1 mask
    strength: 0-1 (0: ì›ë³¸ ìœ ì§€, 1: ì™„ì „ decontaminate)
    """
    mask_expanded = np.clip(mask_float + 0.15, 0, 1)
    blurred_img = cv2.GaussianBlur(img_np, (5, 5), 0)

    decont = img_np * mask_expanded[..., None] + blurred_img * (1 - mask_expanded[..., None]) * strength
    return decont.astype(np.uint8)

# =============================================================
# 8. Guided Filter
# =============================================================
def guided_filter(I, p, r, eps):
    """
    I: guidance image (gray)
    p: mask_float
    r: radius
    eps: regularization
    """
    I = I.astype(np.float32)
    p = p.astype(np.float32)

    mean_I = cv2.boxFilter(I, -1, (r, r))
    mean_p = cv2.boxFilter(p, -1, (r, r))
    corr_I = cv2.boxFilter(I * I, -1, (r, r))
    corr_Ip = cv2.boxFilter(I * p, -1, (r, r))

    var_I = corr_I - mean_I * mean_I
    cov_Ip = corr_Ip - mean_I * mean_p

    a = cov_Ip / (var_I + eps)
    b = mean_p - a * mean_I

    mean_a = cv2.boxFilter(a, -1, (r, r))
    mean_b = cv2.boxFilter(b, -1, (r, r))

    q = mean_a * I + mean_b
    return q


# =============================================================
# best mask ì„ íƒ ìœ í‹¸
# =============================================================

# =============================================================
# 1. ë§ˆìŠ¤í¬ì˜ ì¤‘ì‹¬ë„(centeredness)
# =============================================================
def mask_center_score(mask):
    h, w = mask.shape
    ys, xs = np.where(mask > 0)
    if len(xs) == 0:
        return 0
    
    cx, cy = xs.mean(), ys.mean()
    dx = abs(cx - w / 2) / (w / 2)
    dy = abs(cy - h / 2) / (h / 2)
    dist = (dx + dy) / 2
    # ì¤‘ì•™ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì ìˆ˜ê°€ ë†’ìŒ
    return 1 - dist

# =============================================================
# 2. ë§ˆìŠ¤í¬ ë‚´ë¶€ í‰ê·  ìƒ‰ìƒ ë¶„ì‚°(color variance)
# =============================================================
def color_variance_score(img, mask):
    masked = img[mask > 0]
    if len(masked) == 0:
        return 0
    var = np.var(masked)
    return min(var / 5000, 1.0)

# =============================================================
# 3. ë§ˆìŠ¤í¬ ê²½ê³„ ë³µì¡ë„(edge complexity)
# =============================================================
def edge_complex_score(mask):
    edges = cv2.Canny((mask * 255).astype(np.uint8), 50, 150)
    score = edges.sum() / 255
    return min(score / 5, 1.0)

# =============================================================
# 4. ì¢…í•© ì ìˆ˜ ê³„ì‚°
# =============================================================
def select_best_mask(img_rgb, masks):
    h, w, _ = img_rgb.shape
    best_score = -1e9
    best_mask = None

    for m in masks:
        mask = m["segmentation"]
        if mask.ndim == 3:
            mask = mask[..., 0]
        mask = mask.astype(np.uint8)

        area = m["area"]
        area_ratio = area / (h * w)

        # ë„ˆë¬´ ì‘ê±°ë‚˜ ë„ˆë¬´ í° ì˜ì—­(ë°°ê²½ì¼ í™•ë¥  ë†’ìŒ) ì œê±°
        if area_ratio < 0.01 or area_ratio > 0.9:
            continue

        # 1) ì¤‘ì•™ ì •ë ¬ ì ìˆ˜
        center_score = mask_center_score(mask)

        # 2) ìƒ‰ìƒ ë‹¤ì–‘ì„±
        color_var_score = color_variance_score(img_rgb, mask)

        # 3) ê²½ê³„ ë³µì¡ë„
        edge_score = edge_complex_score(mask)

        # 4) ê²½ê³„ì— ë‹¿ì•„ ìˆëŠ” ë§ˆìŠ¤í¬ íŒ¨ë„í‹°
        ys, xs = np.where(mask > 0)
        if len(xs) > 0:
            border_touch = (
                np.mean(xs < w * 0.05) +
                np.mean(xs > w * 0.95) +
                np.mean(ys < h * 0.05) +
                np.mean(ys > h * 0.95)
            )
        else:
            border_touch = 4

        border_penalty = min(border_touch, 1.0)

        # ìµœì¢… ìŠ¤ì½”ì–´
        total_score = (
            0.45 * center_score +
            0.30 * edge_score +
            0.02 * color_var_score +
            0.10 * area_ratio -
            0.20 * border_penalty
        )

        if total_score > best_score:
            best_score = total_score
            best_mask = mask

    if best_mask is None:
        # fallback: ê°€ì¥ í° ë§ˆìŠ¤í¬ë¼ë„ ì‚¬ìš©
        best_mask = max(masks, key=lambda x: x["area"])["segmentation"]

    return best_mask

# =============================================================
# 5. ëˆ„ë¼ ë¯¸ë¦¬ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜
# =============================================================
def preview_segmentation(original_image: Image.Image) -> dict:
    """
    ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„:
    - mask_array
    - cutout (íˆ¬ëª… ë°°ê²½)
    - overlay (ì›ë³¸ ìœ„ì— ë§ˆìŠ¤í¬ ë°˜íˆ¬ëª… ì˜¤ë²„ë ˆì´)
    ë¥¼ ìƒì„±í•´ì„œ Base64ë¡œ ë°˜í™˜
    """
    model = get_segmentation_singleton()

    mask_array, cutout_image = model.remove_background(original_image)
    mask_image = _mask_array_to_pil(mask_array)

    # 1) cutout PNG (RGBA)
    cutout_rgba = cutout_image.convert("RGBA")
    buf_cutout = BytesIO()
    cutout_rgba.save(buf_cutout, format="PNG")
    cutout_b64 = base64.b64encode(buf_cutout.getvalue()).decode("utf-8")

    # 2) overlay: ì›ë³¸ ìœ„ì— ë§ˆìŠ¤í¬ë¥¼ ë°˜íˆ¬ëª… ìƒ‰ìœ¼ë¡œ ì¹ í•˜ê¸°
    overlay = original_image.convert("RGBA")
    overlay_mask = mask_image.resize(original_image.size).convert("L")

    color_layer = Image.new("RGBA", overlay.size, (0, 255, 0, 120))  # ì—°í•œ ì´ˆë¡ ê³„ì—´
    overlay = Image.composite(color_layer, overlay, overlay_mask)

    buf_overlay = BytesIO()
    overlay.save(buf_overlay, format="PNG")
    overlay_b64 = base64.b64encode(buf_overlay.getvalue()).decode("utf-8")

    # 3) í’ˆì§ˆ heuristic
    H, W = mask_array.shape
    area_ratio = mask_array.sum() / (H * W)
    # ë„ˆë¬´ ì‘ê±°ë‚˜ ë„ˆë¬´ í¬ë©´ warning
    if area_ratio < 0.02:
        quality = "too_small"
    elif area_ratio > 0.85:
        quality = "too_big"
    else:
        quality = "ok"

    return {
        "cutout_b64": cutout_b64,
        "overlay_b64": overlay_b64,
        "area_ratio": area_ratio,
        "quality": quality,
    }

def _mask_array_to_pil(mask_array: np.ndarray) -> Image.Image:
    """SAM ë§ˆìŠ¤í¬(ndarray)ë¥¼ í‘ë°±(L) ëª¨ë“œ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜."""
    scaled = np.clip(mask_array * 255.0, 0, 255).astype("uint8")
    return Image.fromarray(scaled, mode="L")