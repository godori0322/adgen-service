# diffusion_service.py
#
# SD1.5 + ControlNet(Depth) + IP-Adapter(SD1.5)
# - bitsandbytes / Quantization ì „ë¶€ ì œê±°
# - ëª¨ë“  ì„œë¸Œëª¨ë¸(CN, UNet, VAE, TextEncoder)ì„ ê°™ì€ deviceë¡œ ê°•ì œ ì •ë ¬
# - L4 24GB ê¸°ì¤€ fp16 ì‚¬ìš© (CUDA), CPUë©´ fp32



from dotenv import load_dotenv
load_dotenv()

import os
import torch
from PIL import Image, ImageDraw, ImageFont
from diffusers import (
    StableDiffusionControlNetPipeline,
    ControlNetModel,
    StableDiffusionPipeline,
)
from controlnet_aux import MidasDetector
import numpy as np
import base64
from io import BytesIO
from typing import Optional
import random

from backend.app.services.segmentation import ProductSegmentation
from backend.app.core.diffusion_presets import resolve_preset
from backend.app.core.schemas import CompositionMode


# -----------------------------------------------------------------------------#
# ìºì‹œ / ê²½ë¡œ ì„¤ì •                                                              #
# -----------------------------------------------------------------------------#

# main.pyì—ì„œ ì´ë¯¸ ì„¤ì •í–ˆì§€ë§Œ, ë‹¨ë… ì‹¤í–‰/í…ŒìŠ¤íŠ¸ë¥¼ ëŒ€ë¹„í•´ í•œ ë²ˆ ë” ëª…ì‹œ
os.environ.setdefault("HF_HOME", "/home/shared/models")
os.environ.setdefault("DIFFUSERS_CACHE", "/home/shared/models")
os.environ.setdefault("TRANSFORMERS_CACHE", "/home/shared/models")
os.environ.setdefault("TORCH_HOME", "/home/shared/models")
os.environ.setdefault("SAM_MODEL_PATH", "/home/shared/models/sam_vit_b_01ec64.pth")

HF_CACHE_DIR = "/home/shared/models"  # ê³µìš© ìºì‹œ/ëª¨ë¸ ë””ë ‰í„°ë¦¬ ê²½ë¡œ ìƒìˆ˜

# -----------------------------------------------------------------------------#
# ì„¤ì •: SD 1.5 + ControlNet(Depth) + IP-Adapter(SD1.5)                          #
# -----------------------------------------------------------------------------#

SD15_MODEL_ID = "runwayml/stable-diffusion-v1-5"
CONTROLNET_DEPTH_ID = "lllyasviel/control_v11f1p_sd15_depth"
IP_ADAPTER_MODEL_ID = "h94/IP-Adapter"
IP_ADAPTER_SUBFOLDER = "models"
IP_ADAPTER_WEIGHT_NAME = "ip-adapter_sd15.bin"  # SD1.5ìš© IP-Adapter ê°€ì¤‘ì¹˜ íŒŒì¼ëª…

# ì „ì—­ ìºì‹œ
_pipeline = None
_midas_detector = None
_ip_adapter_loaded = False

# í¬ìŠ¤í„°(txt2img)ìš© ë² ì´ìŠ¤ sd 1.5 íŒŒì´í”„ë¼ì¸ ì¶”ê°€
_poster_pipeline = None
# ì „ì—­ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ í•¸ë“¤ëŸ¬ ì¶”ê°€
_segmentation_model : Optional[ProductSegmentation] = None


# -----------------------------------------------------------------------------
# í¬ìŠ¤í„° ìƒì„±ìš© ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ í•¸ë“¤ëŸ¬                                                          
# -----------------------------------------------------------------------------
def _get_segmentation_model() -> ProductSegmentation:
    global _segmentation_model
    if _segmentation_model is None:
        try:
            _segmentation_model = ProductSegmentation()
            print("[Segmentation] MobileSAM ë¡œë“œ ì™„ë£Œ.")
        except Exception as exc:
            print(f"[Segmentation][ERROR] ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {exc}")
            raise RuntimeError("Segmentation model load failed.")
    return _segmentation_model


def _mask_array_to_pil(mask_array: np.ndarray) -> Image.Image:
    """SAM ë§ˆìŠ¤í¬(ndarray)ë¥¼ í‘ë°±(L) ëª¨ë“œ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜."""
    scaled = np.clip(mask_array * 255.0, 0, 255).astype("uint8")
    return Image.fromarray(scaled, mode="L")


def _base64_to_image(base64_string: str) -> Image.Image:
    """
    Base64 ë¬¸ìì—´ì„ PIL Image ê°ì²´ë¡œ ë³€í™˜.
    "data:image/png;base64,..." ê°™ì€ prefixê°€ ìˆì–´ë„ ì²˜ë¦¬.
    """
    try:
        if not base64_string:
            raise ValueError("Empty base64 string")

        if "," in base64_string:
            base64_string = base64_string.split(",", 1)[1]

        image_bytes = base64.b64decode(base64_string)
        return Image.open(BytesIO(image_bytes)).convert("RGB")
    except Exception as e:
        print(f"[ERROR] Base64 to image conversion failed: {e}")
        raise ValueError(f"Invalid Base64 image data provided: {e}")




# -----------------------------------------------------------------------------
# íŒŒì´í”„ë¼ì¸ ë¡œë”© í•¨ìˆ˜                                                          
# -----------------------------------------------------------------------------

def _load_pipeline():
    """
    SD 1.5 + ControlNet(Depth) + IP-Adapter(SD1.5)ë¥¼ ëª¨ë‘ ë¡œë“œí•˜ê³ 
    ì „ì—­ ë³€ìˆ˜ì— ìºì‹±í•˜ëŠ” í•¨ìˆ˜
    """
    global _pipeline, _midas_detector, _ip_adapter_loaded

    if _pipeline is not None:
        return _pipeline

    print("[SD15 Pipeline] Loading base models...")

    # ë””ë°”ì´ìŠ¤ / dtype ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    torch_dtype = torch.float16 if device == "cuda" else torch.float32

    # 1) ControlNet(Depth) ë¡œë“œ
    controlnet_depth = ControlNetModel.from_pretrained(
        CONTROLNET_DEPTH_ID,
        cache_dir=HF_CACHE_DIR,
        torch_dtype=torch_dtype,
    ).to(device)

    # 2) StableDiffusionControlNetPipeline ë¡œë“œ (Base: SD 1.5)
    pipe = StableDiffusionControlNetPipeline.from_pretrained(
        SD15_MODEL_ID,
        controlnet=controlnet_depth,
        cache_dir=HF_CACHE_DIR,
        torch_dtype=torch_dtype,
        safety_checker=None,         # ê´‘ê³ ìš©ì´ë¼ë©´ ë³„ë„ í•„í„°ë§ì—ì„œ ì²˜ë¦¬
        use_safetensors=True,
    ).to(device)

    print(f"[SD15 Pipeline] ê¸°ë³¸ fp16/fp32ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. device={device}, dtype={torch_dtype}")

    # 3) ìŠ¤ì¼€ì¤„ëŸ¬/ë©”ëª¨ë¦¬ ìµœì í™”
    # --> attention_slicingì€ ip-adapterì™€ ì¶©ëŒí•˜ì—¬ ì‚¬ìš©í•˜ì§€ ì•Šê¸°ë¡œ í•¨!
    try:
        if device == "cuda":
            pipe.enable_xformers_memory_efficient_attention()
            print("[SD15 Pipeline] xformers í™œì„±í™”ë¨.")
    except Exception as e:
        print(f"[WARNING] xformers í™œì„±í™” ì‹¤íŒ¨: {e}")


    # VAE slicing/tilingë§Œ ì‚¬ìš© (ë””ì½”ë” ë©”ëª¨ë¦¬ ì ˆê°)
    try:
        pipe.enable_vae_slicing()
        print("[SD15 Pipeline] VAE slicing enabled.")
    except Exception as e:
        print(f"[WARNING] VAE slicing ì„¤ì • ì¤‘ ê²½ê³ : {e}")

    try:
        pipe.enable_vae_tiling()
        print("[SD15 Pipeline] VAE tiling enabled.")
    except Exception as e:
        print(f"[WARNING] VAE tiling ì„¤ì • ì¤‘ ê²½ê³ : {e}")


    # 4) Depth ì „ì²˜ë¦¬ê¸°(Midas) ë¡œë“œ
    print("[Midas Detector] Depth ì „ì²˜ë¦¬ê¸° ë¡œë“œ ì¤‘...")
    _midas_detector = MidasDetector.from_pretrained(
        "lllyasviel/ControlNet",
        cache_dir=HF_CACHE_DIR,
    ).to(device)
    print("[Midas Detector] Depth ì „ì²˜ë¦¬ê¸° ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë¨.")

    # 5) IP-Adapter(SD1.5) ë¡œë“œ
    global _ip_adapter_loaded
    print("[IP-Adapter] SD1.5ìš© IP-Adapter ë¡œë“œ ì¤‘...")
    try:
        pipe.load_ip_adapter(
            IP_ADAPTER_MODEL_ID,          # "h94/IP-Adapter"
            subfolder=IP_ADAPTER_SUBFOLDER,
            weight_name=IP_ADAPTER_WEIGHT_NAME,
        )
        pipe.set_ip_adapter_scale(0.0)    # ê¸°ë³¸ê°’ì€ ë¹„í™œì„±í™”
        _ip_adapter_loaded = True
        print(
            f"[IP-Adapter] {IP_ADAPTER_SUBFOLDER}/{IP_ADAPTER_WEIGHT_NAME} ë¡œë“œ ì„±ê³µ."
        )
    except Exception as e:
        print(f"[WARNING] IP-Adapter ë¡œë“œ ì‹¤íŒ¨: {e}. ìš°ì„  ControlNetë§Œ ì‚¬ìš©í•¨.")
        _ip_adapter_loaded = False

    # 6) ëª¨ë“  ì„œë¸Œëª¨ë¸ ë™ì¼ ë””ë°”ì´ìŠ¤ë¡œ ê°•ì œ ì •ë ¬ (CPU/CUDA í˜¼í•© ë°©ì§€ í•µì‹¬)
    if device == "cuda":
        try:
            pipe.unet.to(device)
            pipe.vae.to(device)
            if hasattr(pipe, "text_encoder"):
                pipe.text_encoder.to(device)
            if hasattr(pipe, "controlnet"):
                pipe.controlnet.to(device)
            print("[SD15 Pipeline] UNet/VAE/TextEncoder/ControlNet ëª¨ë‘ CUDAë¡œ ì •ë ¬ ì™„ë£Œ.")
        except Exception as e:
            print(f"[WARNING] ì„œë¸Œëª¨ë¸ device ì •ë ¬ ì¤‘ ê²½ê³ : {e}")

    _pipeline = pipe
    return _pipeline



#---------------------------------------------------------------------------------- 
# í¬ìŠ¤í„°ìš© íŒŒì´í”„ë¼ì¸ ë¡œë” ì¶”ê°€
#---------------------------------------------------------------------------------- 
def _load_poster_pipeline():
    """
    ControlNet ì—†ì´ ìˆœìˆ˜ StableDiffusionPipeline(SD1.5)ë§Œ ì‚¬ìš©í•˜ëŠ”
    í¬ìŠ¤í„°(txt2img) ìƒì„±ìš© íŒŒì´í”„ë¼ì¸ ë¡œë”.
    """
    global _poster_pipeline

    if _poster_pipeline is not None:
        return _poster_pipeline

    print("[SD15 PosterPipeline] Loading base SD1.5 txt2img pipeline...")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    torch_dtype = torch.float16 if device == "cuda" else torch.float32

    pipe = StableDiffusionPipeline.from_pretrained(
        SD15_MODEL_ID,
        cache_dir=HF_CACHE_DIR,
        torch_dtype=torch_dtype,
        safety_checker=None,   # ì„œë¹„ìŠ¤ ì •ì±…ì— ë§ê²Œ ë‚˜ì¤‘ì— ì¡°ì •
        use_safetensors=True,
    ).to(device)

    print(f"[SD15 PosterPipeline] Loaded. device={device}, dtype={torch_dtype}")

    # ë©”ëª¨ë¦¬ ìµœì í™”(ê°€ëŠ¥í•  ë•Œë§Œ)
    try:
        if device == "cuda":
            pipe.enable_xformers_memory_efficient_attention()
            print("[SD15 PosterPipeline] xformers í™œì„±í™” ì‹œë„.")
    except Exception as e:
        print(f"[WARNING][PosterPipeline] xformers í™œì„±í™” ì‹¤íŒ¨: {e}")

    # ğŸ”¹ Poster íŒŒì´í”„ë¼ì¸ì—ë„ VAE slicing ì ìš©
    try:
        pipe.enable_vae_slicing()
        pipe.enable_vae_tiling()
    except Exception:
        pass

    _poster_pipeline = pipe
    return _poster_pipeline



# -----------------------------------------------------------------------------#
# ë©”ì¸ í•©ì„± í•¨ìˆ˜                                                                #
# -----------------------------------------------------------------------------#

def synthesize_image(
    prompt: str,
    product_image: Image.Image,
    mask_image: Image.Image,
    full_image: Image.Image,
    control_weight: float = 0.5,
    ip_adapter_scale: float = 0.2,
) -> Image.Image:
    """
    SD1.5 + ControlNet(Depth) + IP-Adapter(SD1.5)ë¥¼ ì‚¬ìš©í•´ì„œ
    'ì›ë³¸ ìƒí’ˆ + ìƒˆ ë°°ê²½'ì´ í•©ì„±ëœ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

    - prompt           : ë°°ê²½/ì¥ë©´ì— ëŒ€í•œ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    - product_image    : ëˆ„ë¼ëœ ì œí’ˆ RGB
    - mask_image       : ì œí’ˆ ì˜ì—­ ë§ˆìŠ¤í¬ (í°ìƒ‰=ìƒí’ˆ, ê²€ì •=ë°°ê²½)
    - full_image       : ë°°ê²½ í¬í•¨ ì›ë³¸ ì´ë¯¸ì§€
    - control_weight   : Depth ControlNet ê°•ë„ (0ì´ë©´ depth ë¹„í™œì„±í™”)
    - ip_adapter_scale : ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ë°˜ì˜ ê°•ë„ (0~1 ê¶Œì¥)
    """
    global _pipeline, _midas_detector, _ip_adapter_loaded

    if _pipeline is None:
        _load_pipeline()

    pipe = _pipeline
    # diffusers 0.30 ì´í›„ì—ëŠ” pipe.deviceê°€ ì—†ì„ ìˆ˜ ìˆì–´ì„œ ë°©ì–´
    device = getattr(pipe, "device", "cuda" if torch.cuda.is_available() else "cpu")

    print(
        f"[SD15 Synthesis] Running pipeline. Depth Weight: {control_weight}, IP Scale: {ip_adapter_scale}, device={device}"
    )

    # ê´‘ê³ ìš© ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
    negative_prompt = (
        "monochrome, lowres, bad anatomy, worst quality, low quality, blurry, "
        "text, logo, watermark, signature, handwriting, caption, blob, melted, "
        "distorted, deformed, out of frame"
    )

    try:
        # --------------------------------------------------------------
        # 1. ì…ë ¥ ì´ë¯¸ì§€ ëª¨ë“œ ì •ë¦¬
        # --------------------------------------------------------------
        if product_image.mode not in ("RGB", "RGBA"):
            product_image = product_image.convert("RGB")
        if full_image.mode not in ("RGB", "RGBA"):
            full_image = full_image.convert("RGB")
        if mask_image.mode not in ("L", "RGB", "RGBA"):
            mask_image = mask_image.convert("L")

        # --------------------------------------------------------------
        # 2. Depth ë§µì€ "ë°°ê²½ í¬í•¨ ì›ë³¸(full_image)"ì—ì„œë§Œ ì¶”ì¶œ
        # --------------------------------------------------------------
        depth_map = None
        if control_weight > 0:
            depth_raw = _midas_detector(
                full_image,              # ëˆ„ë¼ê°€ ì•„ë‹Œ ì›ë³¸ ì‚¬ìš©
                detect_resolution=512,
                image_resolution=768,
            )

            depth_np = np.array(depth_raw)
            depth_np = depth_np - depth_np.min()
            if depth_np.max() > 0:
                depth_np = depth_np / depth_np.max()
            depth_np = (depth_np * 255).astype("uint8")
            depth_map = Image.fromarray(depth_np)

        # --------------------------------------------------------------
        # 3. IP-AdapterëŠ” "ëˆ„ë¼ëœ product_image"ì—ì„œ ìŠ¤íƒ€ì¼/ìƒ‰ê° ê°€ì ¸ì˜¤ê¸°
        # --------------------------------------------------------------
        ip_kwargs = {}
        if _ip_adapter_loaded and ip_adapter_scale > 0:
            pipe.set_ip_adapter_scale(ip_adapter_scale)
            ip_kwargs["ip_adapter_image"] = product_image
            print("[IP-Adapter] ip_adapter_image ë° scale ì„¤ì • ì™„ë£Œ.")
        else:
            pipe.set_ip_adapter_scale(0.0)
            print("[IP-Adapter] ë¹„í™œì„±í™” (ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” scale <= 0).")

        # --------------------------------------------------------------
        # 4. ë‚œìˆ˜ ì‹œë“œ (ì¬í˜„ì„± í™•ë³´ìš©)
        # --------------------------------------------------------------
        generator = torch.Generator(device=device).manual_seed(0)

        # --------------------------------------------------------------
        # 5. íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ (Depth on/off ë¶„ê¸°)
        # --------------------------------------------------------------
        if depth_map is not None:
            print("[Pipeline] Using ControlNet Depth (from full_image)")
        else:
            print("[Pipeline] Depth disabled â†’ txt2img + (optional) IP-Adapter")

        # fp16 on cuda / fp32 on cpu ìë™ ì²˜ë¦¬
        if device == "cuda":
            autocast_ctx = torch.cuda.amp.autocast(dtype=torch.float16)
        else:
            # CPUì—ì„œëŠ” autocast í•„ìš” ì—†ìŒ
            from contextlib import nullcontext
            autocast_ctx = nullcontext()

        with torch.inference_mode(), autocast_ctx:
            if depth_map is not None:
                result = pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=depth_map,  # depth ë§µì„ ControlNet ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
                    controlnet_conditioning_scale=control_weight,
                    guidance_scale=8.0,
                    num_inference_steps=20,
                    generator=generator,
                    **ip_kwargs,
                )
            else:
                result = pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=None,
                    controlnet_conditioning_scale=0.0,
                    guidance_scale=9.0,
                    num_inference_steps=40,
                    generator=generator,
                    **ip_kwargs,
                )

        generated_bg = result.images[0]
        del result

        # --------------------------------------------------------------
        # 6. ìµœì¢… í•©ì„±: product_image + ìƒì„± ë°°ê²½
        # --------------------------------------------------------------
        bg_w, bg_h = generated_bg.size
        fg = product_image.resize((bg_w, bg_h), Image.LANCZOS)
        m = mask_image.resize((bg_w, bg_h), Image.LANCZOS).convert("L")

        final_image = Image.composite(fg, generated_bg, m)

        # ì¤‘ê°„ ì´ë¯¸ì§€/í…ì„œ ì°¸ì¡° ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œ
        del generated_bg, fg, m
        del generator, ip_kwargs, depth_map

        return final_image

    except Exception as e:
        print(f"[ERROR] Image generation failed: {e}")
        raise Exception(f"Image generation failed: {e}")
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            print("[GPU Memory] VRAM cleanup executed.")
            
            


#---------------------------------------------------------------------------------- 
# ì…ë ¥ë°›ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™ë³´ìš© ì´ë¯¸ì§€ ì¶œë ¥ (SD 1.5 íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)
#---------------------------------------------------------------------------------- 

def generate_poster_image(
    prompt: str,
    product_image_bytes: Optional[bytes] = None,
) -> bytes:
    """
    Stable Diffusion 1.5 ìˆœìˆ˜ txt2img íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•´ì„œ
    'í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ í™ë³´ìš© í¬ìŠ¤í„° ì´ë¯¸ì§€'ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.

    í˜„ì¬ ë‹¨ê³„:
    - ControlNet / Depth / ì„¸ê·¸ë©˜í…Œì´ì…˜ ì—†ì´ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìƒì„±
    - ë‚˜ì¤‘ì— product_image_bytesë¥¼ í™œìš©í•´ì„œ IP-Adapter ë“±ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥
    """
    pipe = _load_poster_pipeline()
    device = getattr(pipe, "device", "cuda" if torch.cuda.is_available() else "cpu")
    print(f"[SD15 Poster] Generating poster image. device={device}")

    # ê´‘ê³ ìš© ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
    negative_prompt = (
        "monochrome, lowres, bad anatomy, worst quality, low quality, blurry, "
        "text, logo, watermark, signature, handwriting, caption, blob, melted, "
        "distorted, deformed, out of frame"
    )

    # í˜„ì¬ëŠ” product_image_bytesëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³ , ìˆœìˆ˜ í…ìŠ¤íŠ¸ í¬ìŠ¤í„°ë§Œ ìƒì„±
    # (IP-Adapter í™•ì¥ì€ ë‚˜ì¤‘ ë‹¨ê³„ì—ì„œ ì¶”ê°€)

    # ë‚œìˆ˜ ì‹œë“œ(ì¬í˜„ì„± í™•ë³´ìš©)
    generator = torch.Generator(device=device).manual_seed(0)

    # autocast ì„¤ì • (CUDAì¼ ë•Œë§Œ fp16)
    if device == "cuda":
        autocast_ctx = torch.cuda.amp.autocast(dtype=torch.float16)
    else:
        from contextlib import nullcontext
        autocast_ctx = nullcontext()

    try:
        with torch.inference_mode(), autocast_ctx:
            result = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                guidance_scale=8.0,
                num_inference_steps=30,
                generator=generator,
            )

        img = result.images[0]

        buf = BytesIO()
        img.save(buf, format="PNG")
        buf.seek(0)
        print("[SD15 Poster] Poster image generation success.")
        return buf.getvalue()

    except Exception as e:
        print(f"[ERROR][SD15 Poster] Image generation failed: {e}")
        raise RuntimeError(f"Poster image generation error: {e}")
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            print("[GPU Memory] VRAM cleanup executed (poster).")


def run_auto_synthesis(
    original_image: Image.Image,
    prompt: str,
    mode: CompositionMode = CompositionMode.balanced,
    control_weight: float | None = None,
    ip_adapter_scale: float | None = None,
) -> Image.Image:
    """
    1) ì„¸ê·¸ë©˜í…Œì´ì…˜ (MobileSAM + SAM)
    2) CompositionMode í”„ë¦¬ì…‹ + (ì˜µì…˜) override í•´ì„
    3) synthesize_image í˜¸ì¶œ
    """
    model = _get_segmentation_model()

    # 1) segmentation: ì „ì²´ ì´ë¯¸ì§€ ê¸°ì¤€ìœ¼ë¡œ ëˆ„ë¼ ì¶”ì¶œ
    mask_array, cutout_image = model.remove_background(original_image)

    # 2) ë§ˆìŠ¤í¬/ì œí’ˆ RGB ì¤€ë¹„
    mask_image = _mask_array_to_pil(mask_array)
    product_rgb = cutout_image.convert("RGB")

    # 3) CompositionMode + override -> ìµœì¢… ìˆ˜ì¹˜
    cw, ip = resolve_preset(
        mode=mode,
        override_control=control_weight,
        override_ip=ip_adapter_scale,
    )
    print(f"[Preset] mode={mode}, control={cw}, ip={ip}")

    # 4) diffusion í•©ì„±
    return synthesize_image(
        prompt=prompt,
        product_image=product_rgb,
        mask_image=mask_image,
        full_image=original_image,
        control_weight=cw,
        ip_adapter_scale=ip,
    )




# ì œí’ˆ ì´ë¯¸ì§€ + ì„¸ê·¸ë©˜í…Œì´ì…˜ + í•©ì„± í•¨ìˆ˜ ì¶”ê°€
def generate_poster_with_product_b64(
    prompt: str,
    product_image_b64: str,
    composition_mode: CompositionMode = CompositionMode.balanced,
    control_weight: float | None = None,
    ip_adapter_scale: float | None = None,
) -> bytes:
    """
    Base64 ì œí’ˆ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„
    - ì„¸ê·¸ë©˜í…Œì´ì…˜ + í”„ë¦¬ì…‹ + í•©ì„±ê¹Œì§€ ìˆ˜í–‰í•˜ê³ 
    - ìµœì¢… PNG ë°”ì´íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.

    /api/diffusion/synthesize/auto, /api/ads/generate ì–‘ìª½ì—ì„œ ê³µìš© ì‚¬ìš©.
    """
    original_image = _base64_to_image(product_image_b64)

    # í•©ì„± ì‹¤í–‰
    final_image_pil = run_auto_synthesis(
        original_image=original_image,
        prompt=prompt,
        mode=composition_mode,
        control_weight=control_weight,
        ip_adapter_scale=ip_adapter_scale,
    )

    buf = BytesIO()
    final_image_pil.save(buf, format="PNG")
    buf.seek(0)
    return buf.getvalue()

